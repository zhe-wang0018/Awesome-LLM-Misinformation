# üõ°Ô∏èAwesome LLM Misinformationüõ°Ô∏è[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/zhe-wang0018/Awesome-LLM-Misonformation?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/zhe-wang0018/Awesome-LLM-Misonformation?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/zhe-wang0018/Awesome-LLM-Misonformation?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/zhe-wang0018/Awesome-LLM-Misonformation?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>

## ü§óIntroduction


**Welcome to our Awesome-llm-misinformation repository!** 

**This is a list of LLM Misinformation papers, articles, and resources focused on Large Language Models (LLMs). We thank ‚úâÔ∏è: [ydyjya](https://github.com/ydyjya) ‚û°Ô∏è zhouzhenhong@bupt.edu.cn for his template!** 



**üî• News**

- 2024.09 We create this repository!

**üßë‚Äçüíª Our Work**

We've curated a collection of the latest üòã, most comprehensive üòé, and most valuable ü§© resources on large language model misinformation. 

## üöÄTable of Contents

- [üõ°Ô∏èAwesome LLM Misinformationüõ°Ô∏è](#Ô∏èawesome-llm-misinformation)
  - [Introduction](#introduction)
  - [Table of Contents](#table-of-contents)
  - [LLM-generated Misinformation Detection](#misinformation-detection)
  - [Datasets \& Benchmark](#datasets--benchmark)
  - [Scholars](#-scholars-)
  - [Author](#author)

---
## üì∞LLM-generated Misinformation Detection 


### Papers of unintended misinformation (hallucinations)
| Date  |           Institute            | Publication |                                                                    Paper                                                                     |
|:-----:|:------------------------------:|:-----------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      |   ACL2022   |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |    arxiv    | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |    arxiv    |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### Papers of intended misinformation


| Date  |           Institute            | Publication |                                                                    Paper                                                                     |
|:-----:|:------------------------------:|:-----------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 23.11 |      Cornell University        |  NAACL2024  |                         [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                 |
| 23.09 | Chinese Academy of Science     |   AAAI2024  | [Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection](https://arxiv.org/abs/2309.12247)              |
| 24.05 | Hong Kong Baptist University   |    WWW2024  |         [Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom](https://arxiv.org/abs/2405.03371)     |
| 24.05 | Chinese Academy of Sciences    |    arxiv    |    [Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models](https://arxiv.org/abs/2405.16631)   |
| 24.02 | City University of Hong Kong   |   ACL2024   |    [TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection](https://arxiv.org/abs/2402.07776)   |

---
## Datasets & Benchmark

### Papers
| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |
| 23.08 |The Hong Kong Polytechnic University|       arxiv       | [MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models](https://www.arxiv.org/abs/2408.11871) |


---
## Author

**If you have any questions, please contact our authors!**

‚úâÔ∏è: [Lionel Z. WANG](https://zhe-wang0018.github.io/]) ‚û°Ô∏è zhe-leo.wang@connect.polyu.hk


---

**[‚¨Ü Back to ToC](#table-of-contents)**

