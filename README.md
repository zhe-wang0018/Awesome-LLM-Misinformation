# ğŸ›¡ï¸Awesome LLM MisinformationğŸ›¡ï¸[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/issues/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub issues"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/ydyjya/Awesome-LLM-Safety?style=flat-square&logo=github" alt="GitHub Last commit"></a>
</p>
<div align="center">

English | [ä¸­æ–‡](README_cn.md)

</div>

## ğŸ¤—Introduction


**Welcome to our Awesome-llm-misinformation repository!** 

**This is a list of LLM Misinformation papers, articles, and resources focused on Large Language Models (LLMs). We thank âœ‰ï¸: [ydyjya](https://github.com/ydyjya) â¡ï¸ zhouzhenhong@bupt.edu.cn for his template!** 



**ğŸ”¥ News**

- 2024.09 We create this repository!

**ğŸ§‘â€ğŸ’» Our Work**

We've curated a collection of the latest ğŸ˜‹, most comprehensive ğŸ˜, and most valuable ğŸ¤© resources on large language model misinformation. 

## ğŸš€Table of Contents

- [ğŸ›¡ï¸Awesome LLM MisinformationğŸ›¡ï¸](#ï¸awesome-llm-misinformation)
  - [Introduction](#introduction)
  - [Table of Contents](#table-of-contents)
  - [LLM-generated Misinformation Detection](#misinformation-detection)
  - [Datasets \& Benchmark](#datasets--benchmark)
    - [ğŸ“‘Papers](#papers-4)
    - [ğŸ“–Tutorials, Articles, Presentations and Talks](#tutorials-articles-presentations-and-talks-5)
    - [ğŸ“šResourceğŸ“š](#resource)
    - [Other](#other-5)
  - [Scholars](#-scholars-)
  - [Author](#author)

---
## ğŸ“°LLM-generated Misinformation Detection 


### Papers of unintended misinformation (hallucinations)
| Date  |           Institute            | Publication |                                                                    Paper                                                                     |
|:-----:|:------------------------------:|:-----------:|:--------------------------------------------------------------------------------------------------------------------------------------------:|
| 21.09 |      University of Oxford      |   ACL2022   |                         [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                          |
| 23.11 | Harbin Institute of Technology |    arxiv    | [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232) |
| 23.11 |    Arizona State University    |    arxiv    |                      [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                      |

### Papers of intended misinformation

| Date  |    Type    |          Title           |                                URL                                |
|:-----:|:----------:|:------------------------:|:-----------------------------------------------------------------:|
| 23.07 | Repository | llm-hallucination-survey | [link](https://github.com/HillZhang1999/llm-hallucination-survey) |
| 23.10 | Repository |  LLM-Factuality-Survey   |   [link](https://github.com/wangcunxiang/LLM-Factuality-Survey)   |
| 23.10 | Tutorials  |    Awesome-LLM-Safety    |       [link](https://github.com/ydyjya/Awesome-LLM-Safety)        |


---
## Datasets & Benchmark

### Papers
| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |
| 23.08 |The Hong Kong Polytechnic University|       arxiv       | [MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models](https://www.arxiv.org/abs/2408.11871) |


---
## Author

**If you have any questions, please contact our authors!**

âœ‰ï¸: [Lionel Z. WANG](https://zhe-wang0018.github.io/]) â¡ï¸ zhe-leo.wang@connect.polyu.hk


---

**[â¬† Back to ToC](#table-of-contents)**

